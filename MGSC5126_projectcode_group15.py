# -*- coding: utf-8 -*-
"""MGSC5126_ProjectCode_Group15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rokBrnz_q2gnghz78Jf37A1CLft9GCrb
"""

# Commented out IPython magic to ensure Python compatibility.
#Installed the required library in Colab Environment
!pip install pandas
!pip install matplotlib
!pip install seaborn

#Import these libraries
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()

#Read the dataset and check the columns and first 5 rows.
df = pd.read_csv('/content/taxi-fares.csv')
df.head()

#Check the number of rows and column
df.shape

#Look for any missing values by comparing the rows and number of values for each column
df.info()

#To check and have clear information of the count of passengers in trips
sns.countplot(x=df['passenger_count'])

#Keep rows with passenger_count = 1, remove multiple passenger_count rows and other irrelevant column such as key
df = df[df['passenger_count'] == 1]
df = df.drop(['key', 'passenger_count'], axis=1)
df.head()

df.shape

numeric_df = df.select_dtypes(include=['number'])
corr_matrix = numeric_df.corr()
fare_corr = corr_matrix['fare_amount'].sort_values(ascending=False)
print(fare_corr)

#Feature Engineering step
#Use the attributes such as longitude and latitude to compute important feature such as distance.
#use the information in pickup_datetime to draw important features such as day of week, pick up time
import datetime
from math import sqrt

for i, row in df.iterrows():
    dt = datetime.datetime.strptime(row['pickup_datetime'], '%Y-%m-%d %H:%M:%S UTC')
    df.at[i, 'day_of_week'] = dt.weekday()
    df.at[i, 'pickup_time'] = dt.hour
    x = (row['dropoff_longitude'] - row['pickup_longitude']) * 54.6 # 1 degree == 54.6 miles
    y = (row['dropoff_latitude'] - row['pickup_latitude']) * 69.0   # 1 degree == 69 miles
    distance = sqrt(x**2 + y**2)
    df.at[i, 'distance'] = distance

df.head()

#Remove the irrelevant columns
df.drop(columns=['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'], inplace=True)
df.head()

corr_matrix = df.corr()
corr_matrix["fare_amount"].sort_values(ascending=False)

#understanding the final subset
df.describe()

#Keep only the rows where the 'distance' column values are between 1 and 10 and 'fare_amount' column values are between 0 and 50
df = df[(df['distance'] > 1.0) & (df['distance'] < 10.0)]
df = df[(df['fare_amount'] > 0.0) & (df['fare_amount'] < 50.0)]
df.shape

corr_matrix = df.corr()
corr_matrix["fare_amount"].sort_values(ascending=False)

!pip install lightgbm

from sklearn.model_selection import train_test_split

x = df.drop(['fare_amount'], axis=1)
y = df['fare_amount']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt
from sklearn.model_selection import cross_val_score
import lightgbm as lgb

# 1. Linear Regression
linear_model = LinearRegression()
linear_model.fit(x_train, y_train)
linear_predictions = linear_model.predict(x_test)

linear_mae = mean_absolute_error(y_test, linear_predictions)
linear_rmse = sqrt(mean_squared_error(y_test, linear_predictions))
linear_mse = mean_squared_error(y_test, linear_predictions)
linear_r2 = r2_score(y_test, linear_predictions)

linear_cv_score = cross_val_score(linear_model, x, y, cv=5)

# 2. Random Forest
rf_model = RandomForestRegressor()
rf_model.fit(x_train, y_train)
rf_predictions = rf_model.predict(x_test)

rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_rmse = sqrt(mean_squared_error(y_test, rf_predictions))
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

rf_cv_score = cross_val_score(rf_model, x, y, cv=5)

# 3. Combined Model (Linear Regression and Random Forest)
combined_predictions = (linear_model.predict(x_test) + rf_model.predict(x_test)) / 2

combined_mae = mean_absolute_error(y_test, combined_predictions)
combined_rmse = sqrt(mean_squared_error(y_test, combined_predictions))
combined_mse = mean_squared_error(y_test, combined_predictions)
combined_r2 = r2_score(y_test, combined_predictions)

combined_cv_score = cross_val_score(LinearRegression(), x, y, cv=5)  # Using Linear Regression for cross-validation

# 4. LightGBM
lgb_model = lgb.LGBMRegressor()
lgb_model.fit(x_train, y_train)
lgb_predictions = lgb_model.predict(x_test)

lgb_mae = mean_absolute_error(y_test, lgb_predictions)
lgb_rmse = sqrt(mean_squared_error(y_test, lgb_predictions))
lgb_mse = mean_squared_error(y_test, lgb_predictions)
lgb_r2 = r2_score(y_test, lgb_predictions)

lgb_cv_score = cross_val_score(lgb_model, x, y, cv=5)

# Print results
print("Linear Regression Model Metrics:")
print("MAE:", linear_mae)
print("RMSE:", linear_rmse)
print("MSE:", linear_mse)
print("R-squared:", linear_r2)
print("Cross-validated R-squared score for Linear Regression Model:", linear_cv_score.mean())
print()

print("Random Forest Model Metrics:")
print("MAE:", rf_mae)
print("RMSE:", rf_rmse)
print("MSE:", rf_mse)
print("R-squared:", rf_r2)
print("Cross-validated R-squared score for Random Forest Model:", rf_cv_score.mean())
print()

print("Combined Model Metrics:")
print("MAE:", combined_mae)
print("RMSE:", combined_rmse)
print("MSE:", combined_mse)
print("R-squared:", combined_r2)
print("Cross-validated R-squared score for Combined Model:", combined_cv_score.mean())
print()

print("LightGBM Model Metrics:")
print("MAE:", lgb_mae)
print("RMSE:", lgb_rmse)
print("MSE:", lgb_mse)
print("R-squared:", lgb_r2)
print("Cross-validated R-squared score for LightGBM Model:", lgb_cv_score.mean())